{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from sklearn.metrics import *\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we train the LR model on regular dreaddit dataset\n",
    "train_data = pd.read_csv('dreaddit-train.csv')\n",
    "test_data = pd.read_csv('dreaddit-test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>social_karma</th>\n",
       "      <th>syntax_ari</th>\n",
       "      <th>lex_liwc_WC</th>\n",
       "      <th>lex_liwc_Analytic</th>\n",
       "      <th>lex_liwc_Clout</th>\n",
       "      <th>lex_liwc_Authentic</th>\n",
       "      <th>lex_liwc_Tone</th>\n",
       "      <th>lex_liwc_WPS</th>\n",
       "      <th>lex_liwc_Sixltr</th>\n",
       "      <th>lex_liwc_Dic</th>\n",
       "      <th>...</th>\n",
       "      <th>lex_dal_min_activation</th>\n",
       "      <th>lex_dal_min_imagery</th>\n",
       "      <th>lex_dal_avg_activation</th>\n",
       "      <th>lex_dal_avg_imagery</th>\n",
       "      <th>lex_dal_avg_pleasantness</th>\n",
       "      <th>social_upvote_ratio</th>\n",
       "      <th>social_num_comments</th>\n",
       "      <th>syntax_fk_grade</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>roberta_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1.806818</td>\n",
       "      <td>116</td>\n",
       "      <td>72.64</td>\n",
       "      <td>15.04</td>\n",
       "      <td>89.26</td>\n",
       "      <td>1.00</td>\n",
       "      <td>29.00</td>\n",
       "      <td>12.93</td>\n",
       "      <td>87.07</td>\n",
       "      <td>...</td>\n",
       "      <td>1.1250</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.77000</td>\n",
       "      <td>1.52211</td>\n",
       "      <td>1.89556</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1</td>\n",
       "      <td>3.253573</td>\n",
       "      <td>-0.002742</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>9.429737</td>\n",
       "      <td>109</td>\n",
       "      <td>79.08</td>\n",
       "      <td>76.85</td>\n",
       "      <td>56.75</td>\n",
       "      <td>98.18</td>\n",
       "      <td>27.25</td>\n",
       "      <td>21.10</td>\n",
       "      <td>87.16</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.69586</td>\n",
       "      <td>1.62045</td>\n",
       "      <td>1.88919</td>\n",
       "      <td>0.65</td>\n",
       "      <td>2</td>\n",
       "      <td>8.828316</td>\n",
       "      <td>0.292857</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>7.769821</td>\n",
       "      <td>167</td>\n",
       "      <td>33.80</td>\n",
       "      <td>76.38</td>\n",
       "      <td>86.24</td>\n",
       "      <td>25.77</td>\n",
       "      <td>33.40</td>\n",
       "      <td>17.37</td>\n",
       "      <td>91.02</td>\n",
       "      <td>...</td>\n",
       "      <td>1.1429</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.83088</td>\n",
       "      <td>1.58108</td>\n",
       "      <td>1.85828</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0</td>\n",
       "      <td>7.841667</td>\n",
       "      <td>0.011894</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2.667798</td>\n",
       "      <td>273</td>\n",
       "      <td>2.98</td>\n",
       "      <td>15.25</td>\n",
       "      <td>95.42</td>\n",
       "      <td>79.26</td>\n",
       "      <td>54.60</td>\n",
       "      <td>8.06</td>\n",
       "      <td>98.90</td>\n",
       "      <td>...</td>\n",
       "      <td>1.1250</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.75356</td>\n",
       "      <td>1.52114</td>\n",
       "      <td>1.98848</td>\n",
       "      <td>0.50</td>\n",
       "      <td>5</td>\n",
       "      <td>4.104027</td>\n",
       "      <td>0.141671</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24</td>\n",
       "      <td>7.554238</td>\n",
       "      <td>89</td>\n",
       "      <td>32.22</td>\n",
       "      <td>28.71</td>\n",
       "      <td>84.01</td>\n",
       "      <td>1.00</td>\n",
       "      <td>17.80</td>\n",
       "      <td>31.46</td>\n",
       "      <td>88.76</td>\n",
       "      <td>...</td>\n",
       "      <td>1.1250</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.77644</td>\n",
       "      <td>1.64872</td>\n",
       "      <td>1.81456</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>7.910952</td>\n",
       "      <td>-0.204167</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2833</th>\n",
       "      <td>13</td>\n",
       "      <td>-1.369333</td>\n",
       "      <td>89</td>\n",
       "      <td>19.41</td>\n",
       "      <td>99.00</td>\n",
       "      <td>37.57</td>\n",
       "      <td>99.00</td>\n",
       "      <td>17.80</td>\n",
       "      <td>5.62</td>\n",
       "      <td>97.75</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.71133</td>\n",
       "      <td>1.45301</td>\n",
       "      <td>2.00304</td>\n",
       "      <td>0.84</td>\n",
       "      <td>16</td>\n",
       "      <td>0.254444</td>\n",
       "      <td>0.552066</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2834</th>\n",
       "      <td>33</td>\n",
       "      <td>9.425478</td>\n",
       "      <td>135</td>\n",
       "      <td>40.97</td>\n",
       "      <td>4.45</td>\n",
       "      <td>98.01</td>\n",
       "      <td>1.00</td>\n",
       "      <td>27.00</td>\n",
       "      <td>17.78</td>\n",
       "      <td>96.30</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.65003</td>\n",
       "      <td>1.56842</td>\n",
       "      <td>1.81527</td>\n",
       "      <td>0.96</td>\n",
       "      <td>6</td>\n",
       "      <td>8.640664</td>\n",
       "      <td>-0.220370</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2835</th>\n",
       "      <td>2</td>\n",
       "      <td>11.060675</td>\n",
       "      <td>134</td>\n",
       "      <td>79.52</td>\n",
       "      <td>97.34</td>\n",
       "      <td>2.27</td>\n",
       "      <td>80.01</td>\n",
       "      <td>22.33</td>\n",
       "      <td>25.37</td>\n",
       "      <td>84.33</td>\n",
       "      <td>...</td>\n",
       "      <td>1.1250</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.79768</td>\n",
       "      <td>1.49074</td>\n",
       "      <td>1.92286</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>9.951524</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2836</th>\n",
       "      <td>4</td>\n",
       "      <td>2.421912</td>\n",
       "      <td>68</td>\n",
       "      <td>29.74</td>\n",
       "      <td>61.58</td>\n",
       "      <td>21.06</td>\n",
       "      <td>25.77</td>\n",
       "      <td>13.60</td>\n",
       "      <td>16.18</td>\n",
       "      <td>92.65</td>\n",
       "      <td>...</td>\n",
       "      <td>1.1429</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.71642</td>\n",
       "      <td>1.57627</td>\n",
       "      <td>1.89972</td>\n",
       "      <td>0.75</td>\n",
       "      <td>7</td>\n",
       "      <td>4.036765</td>\n",
       "      <td>0.159722</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2837</th>\n",
       "      <td>2</td>\n",
       "      <td>0.835254</td>\n",
       "      <td>57</td>\n",
       "      <td>1.00</td>\n",
       "      <td>29.92</td>\n",
       "      <td>28.23</td>\n",
       "      <td>1.00</td>\n",
       "      <td>14.25</td>\n",
       "      <td>7.02</td>\n",
       "      <td>98.25</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.68891</td>\n",
       "      <td>1.44615</td>\n",
       "      <td>1.89707</td>\n",
       "      <td>0.76</td>\n",
       "      <td>2</td>\n",
       "      <td>2.412000</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2838 rows Ã— 109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      social_karma  syntax_ari  lex_liwc_WC  lex_liwc_Analytic  \\\n",
       "0                5    1.806818          116              72.64   \n",
       "1                4    9.429737          109              79.08   \n",
       "2                2    7.769821          167              33.80   \n",
       "3                0    2.667798          273               2.98   \n",
       "4               24    7.554238           89              32.22   \n",
       "...            ...         ...          ...                ...   \n",
       "2833            13   -1.369333           89              19.41   \n",
       "2834            33    9.425478          135              40.97   \n",
       "2835             2   11.060675          134              79.52   \n",
       "2836             4    2.421912           68              29.74   \n",
       "2837             2    0.835254           57               1.00   \n",
       "\n",
       "      lex_liwc_Clout  lex_liwc_Authentic  lex_liwc_Tone  lex_liwc_WPS  \\\n",
       "0              15.04               89.26           1.00         29.00   \n",
       "1              76.85               56.75          98.18         27.25   \n",
       "2              76.38               86.24          25.77         33.40   \n",
       "3              15.25               95.42          79.26         54.60   \n",
       "4              28.71               84.01           1.00         17.80   \n",
       "...              ...                 ...            ...           ...   \n",
       "2833           99.00               37.57          99.00         17.80   \n",
       "2834            4.45               98.01           1.00         27.00   \n",
       "2835           97.34                2.27          80.01         22.33   \n",
       "2836           61.58               21.06          25.77         13.60   \n",
       "2837           29.92               28.23           1.00         14.25   \n",
       "\n",
       "      lex_liwc_Sixltr  lex_liwc_Dic  ...  lex_dal_min_activation  \\\n",
       "0               12.93         87.07  ...                  1.1250   \n",
       "1               21.10         87.16  ...                  1.0000   \n",
       "2               17.37         91.02  ...                  1.1429   \n",
       "3                8.06         98.90  ...                  1.1250   \n",
       "4               31.46         88.76  ...                  1.1250   \n",
       "...               ...           ...  ...                     ...   \n",
       "2833             5.62         97.75  ...                  1.0000   \n",
       "2834            17.78         96.30  ...                  1.0000   \n",
       "2835            25.37         84.33  ...                  1.1250   \n",
       "2836            16.18         92.65  ...                  1.1429   \n",
       "2837             7.02         98.25  ...                  1.0000   \n",
       "\n",
       "      lex_dal_min_imagery  lex_dal_avg_activation  lex_dal_avg_imagery  \\\n",
       "0                     1.0                 1.77000              1.52211   \n",
       "1                     1.0                 1.69586              1.62045   \n",
       "2                     1.0                 1.83088              1.58108   \n",
       "3                     1.0                 1.75356              1.52114   \n",
       "4                     1.0                 1.77644              1.64872   \n",
       "...                   ...                     ...                  ...   \n",
       "2833                  1.0                 1.71133              1.45301   \n",
       "2834                  1.0                 1.65003              1.56842   \n",
       "2835                  1.0                 1.79768              1.49074   \n",
       "2836                  1.0                 1.71642              1.57627   \n",
       "2837                  1.0                 1.68891              1.44615   \n",
       "\n",
       "      lex_dal_avg_pleasantness  social_upvote_ratio  social_num_comments  \\\n",
       "0                      1.89556                 0.86                    1   \n",
       "1                      1.88919                 0.65                    2   \n",
       "2                      1.85828                 0.67                    0   \n",
       "3                      1.98848                 0.50                    5   \n",
       "4                      1.81456                 1.00                    1   \n",
       "...                        ...                  ...                  ...   \n",
       "2833                   2.00304                 0.84                   16   \n",
       "2834                   1.81527                 0.96                    6   \n",
       "2835                   1.92286                 1.00                    1   \n",
       "2836                   1.89972                 0.75                    7   \n",
       "2837                   1.89707                 0.76                    2   \n",
       "\n",
       "      syntax_fk_grade  sentiment  roberta_prediction  \n",
       "0            3.253573  -0.002742                   1  \n",
       "1            8.828316   0.292857                   0  \n",
       "2            7.841667   0.011894                   1  \n",
       "3            4.104027   0.141671                   1  \n",
       "4            7.910952  -0.204167                   1  \n",
       "...               ...        ...                 ...  \n",
       "2833         0.254444   0.552066                   0  \n",
       "2834         8.640664  -0.220370                   1  \n",
       "2835         9.951524   0.045455                   0  \n",
       "2836         4.036765   0.159722                   0  \n",
       "2837         2.412000   0.016667                   1  \n",
       "\n",
       "[2838 rows x 109 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets check what features we'll use in the model\n",
    "test = train_data.iloc[:,8::]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets all featurs, including the roberta ones if the dataset has them\n",
    "def get_examples_features_as_np(df, col_start = 0, col_end = None):\n",
    "    if (col_end is None):\n",
    "        col_end = len(df)\n",
    "    df_cols = dtrain_data2 = get_examples_features_as_np(train_data, 8)\n",
    "test_data2 = get_examples_features_as_np(test_data, 8)f.iloc[:,col_start:col_end]\n",
    "    return df_cols.to_numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "LR - all features, no Roberta predictions\n",
      "Accuracy:  0.7482517482517482\n",
      "Recall:  0.7859078590785907\n",
      "Precision:  0.7416879795396419\n",
      "F1:  0.763157894736842\n",
      "\n",
      "\n",
      "-----------------------------\n",
      "LR - all features + base Roberta predictions\n",
      "Accuracy:  0.8111888111888111\n",
      "Recall:  0.8807588075880759\n",
      "Precision:  0.78125\n",
      "F1:  0.8280254777070063\n",
      "\n",
      "\n",
      "-----------------------------\n",
      "LR - all features + pretrained Roberta (our dataset) predictions\n",
      "Accuracy:  0.820979020979021\n",
      "Recall:  0.907859078590786\n",
      "Precision:  0.7808857808857809\n",
      "F1:  0.8395989974937343\n",
      "\n",
      "\n",
      "-----------------------------\n",
      "LR - all features + pretrained Roberta (mental health) predictions\n",
      "Accuracy:  0.7440559440559441\n",
      "Recall:  0.8292682926829268\n",
      "Precision:  0.7183098591549296\n",
      "F1:  0.769811320754717\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_names = [\"LR - all features, no Roberta predictions\",\n",
    "               \"LR - all features + base Roberta predictions\",\n",
    "               \"LR - all features + pretrained Roberta (our dataset) predictions\",\n",
    "               \"LR - all features + pretrained Roberta (mental health) predictions\"]\n",
    "\n",
    "model_train_datasets = [\"dreaddit-train.csv\",\n",
    "                        \"train_pred_base_roberta.csv\",\n",
    "                        \"train_pred_roberta_our_scraped_data.csv\",\n",
    "                        \"train_pred_mental_health.csv\"]\n",
    "\n",
    "model_test_datasets = [\"dreaddit-test.csv\",\n",
    "                       \"test_pred_base_roberta.csv\",\n",
    "                       \"test_pred_roberta_our_scraped_data.csv\",\n",
    "                        \"test_pred_mental_health.csv\"]\n",
    "\n",
    "def test_lr_roberta_all_features(model_name, dataset_path_train = None,dataset_path_test = None):\n",
    "    if (dataset_path_train is None or dataset_path_test is None):\n",
    "        raise RuntimeException(\"Error! Dataset must be provided\")\n",
    "    train_data = pd.read_csv(dataset_path_train)\n",
    "    test_data = pd.read_csv(dataset_path_test)\n",
    "    \n",
    "    train_labels = train_data[[\"label\"]].to_numpy().transpose()[0]\n",
    "    test_labels = test_data[[\"label\"]].to_numpy().transpose()[0]\n",
    "    \n",
    "    # include all features in tranining, including roberta predictions\n",
    "    train_data_np = get_examples_features_as_np(train_data, 8)\n",
    "    test_data_np = get_examples_features_as_np(test_data, 8)\n",
    "    \n",
    "    model = LogisticRegression(max_iter=1000000, solver=\"lbfgs\")\n",
    "    model.fit(train_data_np, train_labels)\n",
    "    \n",
    "    y_pred = model.predict(test_data_np)\n",
    "    \n",
    "    acc = accuracy_score(test_labels, y_pred)\n",
    "    rec = recall_score(test_labels, y_pred, zero_division=1)\n",
    "    prec = precision_score(test_labels, y_pred, zero_division=1)\n",
    "    f1 = f1_score(test_labels, y_pred, zero_division=1)\n",
    "\n",
    "    print(\"-----------------------------\")\n",
    "    print(model_name)\n",
    "    print(\"Accuracy: \", acc)\n",
    "    print(\"Recall: \", rec)\n",
    "    print(\"Precision: \", prec)\n",
    "    print(\"F1: \", f1)\n",
    "    print(\"\\n\")\n",
    "\n",
    "for i in range(4):\n",
    "    test_lr_roberta_all_features(model_names[i], model_train_datasets[i], model_test_datasets[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interesting - model works better when we include all 108 features than\n",
    "# when we include only the best LIWC features -> comment on this in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "LR - best LIWC features, no Roberta predictions\n",
      "Accuracy:  0.5986013986013986\n",
      "Recall:  0.6476964769647696\n",
      "Precision:  0.6035353535353535\n",
      "F1:  0.6248366013071894\n",
      "\n",
      "\n",
      "-----------------------------\n",
      "LR - best LIWC features + base Roberta predictions\n",
      "Accuracy:  0.8111888111888111\n",
      "Recall:  0.8861788617886179\n",
      "Precision:  0.7785714285714286\n",
      "F1:  0.8288973384030418\n",
      "\n",
      "\n",
      "-----------------------------\n",
      "LR - best LIWC features + pretrained Roberta (our dataset) predictions\n",
      "Accuracy:  0.8167832167832167\n",
      "Recall:  0.9105691056910569\n",
      "Precision:  0.7741935483870968\n",
      "F1:  0.8368617683686178\n",
      "\n",
      "\n",
      "-----------------------------\n",
      "LR - best LIWC features + pretrained Roberta (mental health) predictions\n",
      "Accuracy:  0.7034965034965035\n",
      "Recall:  0.94579945799458\n",
      "Precision:  0.6451016635859519\n",
      "F1:  0.767032967032967\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_names = [\"LR - best LIWC features, no Roberta predictions\",\n",
    "               \"LR - best LIWC features + base Roberta predictions\",\n",
    "               \"LR - best LIWC features + pretrained Roberta (our dataset) predictions\",\n",
    "               \"LR - best LIWC features + pretrained Roberta (mental health) predictions\"]\n",
    "\n",
    "model_train_datasets = [\"dreaddit-train.csv\",\n",
    "                        \"train_pred_base_roberta.csv\",\n",
    "                        \"train_pred_roberta_our_scraped_data.csv\",\n",
    "                        \"train_pred_mental_health.csv\"]\n",
    "\n",
    "model_test_datasets = [\"dreaddit-test.csv\",\n",
    "                       \"test_pred_base_roberta.csv\",\n",
    "                       \"test_pred_roberta_our_scraped_data.csv\",\n",
    "                        \"test_pred_mental_health.csv\"]\n",
    "\n",
    "# test the LR on best performing LIWC features and Roberta predictions (if applicable)\n",
    "# features are: (Work, Adverb, Shehe, Drives, Ipron, Period, Swear) \n",
    "def test_lr_roberta_best_features(model_name, dataset_path_train = None,dataset_path_test = None):\n",
    "    if (dataset_path_train is None or dataset_path_test is None):\n",
    "        raise RuntimeException(\"Error! Dataset must be provided\")\n",
    "    train_data = pd.read_csv(dataset_path_train)\n",
    "    test_data = pd.read_csv(dataset_path_test)\n",
    "    \n",
    "    train_labels = train_data[[\"label\"]].to_numpy().transpose()[0]\n",
    "    test_labels = test_data[[\"label\"]].to_numpy().transpose()[0]\n",
    "    \n",
    "    if (model_name != \"LR - best LIWC features, no Roberta predictions\"):\n",
    "        train_data = train_data[['lex_liwc_work', 'lex_liwc_adverb','lex_liwc_shehe',\n",
    "                             'lex_liwc_drives', 'lex_liwc_ipron', 'lex_liwc_Period',\n",
    "                            'lex_liwc_swear', 'roberta_prediction']]\n",
    "    \n",
    "        test_data = test_data[['lex_liwc_work', 'lex_liwc_adverb','lex_liwc_shehe',\n",
    "                             'lex_liwc_drives', 'lex_liwc_ipron', 'lex_liwc_Period',\n",
    "                            'lex_liwc_swear', 'roberta_prediction']]\n",
    "    else:\n",
    "        train_data = train_data[['lex_liwc_work', 'lex_liwc_adverb','lex_liwc_shehe',\n",
    "                                 'lex_liwc_drives', 'lex_liwc_ipron', 'lex_liwc_Period',\n",
    "                                'lex_liwc_swear']]\n",
    "\n",
    "        test_data = test_data[['lex_liwc_work', 'lex_liwc_adverb','lex_liwc_shehe',\n",
    "                                 'lex_liwc_drives', 'lex_liwc_ipron', 'lex_liwc_Period',\n",
    "                                'lex_liwc_swear']]\n",
    "\n",
    "    # include all features in tranining, including roberta predictions\n",
    "    \n",
    "    train_data_np = train_data.to_numpy()\n",
    "    test_data_np = test_data.to_numpy()\n",
    "    \n",
    "    model = LogisticRegression(max_iter=1000000, solver=\"lbfgs\")\n",
    "    model.fit(train_data_np, train_labels)\n",
    "    \n",
    "    y_pred = model.predict(test_data_np)\n",
    "    \n",
    "    acc = accuracy_score(test_labels, y_pred)\n",
    "    rec = recall_score(test_labels, y_pred, zero_division=1)\n",
    "    prec = precision_score(test_labels, y_pred, zero_division=1)\n",
    "    f1 = f1_score(test_labels, y_pred, zero_division=1)\n",
    "\n",
    "    print(\"-----------------------------\")\n",
    "    print(model_name)\n",
    "    print(\"Accuracy: \", acc)\n",
    "    print(\"Recall: \", rec)\n",
    "    print(\"Precision: \", prec)\n",
    "    print(\"F1: \", f1)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "for i in range(4):\n",
    "    test_lr_roberta_best_features(model_names[i], model_train_datasets[i], model_test_datasets[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
