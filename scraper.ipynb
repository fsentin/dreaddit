{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b203c2a",
   "metadata": {},
   "source": [
    "# Subreddits scraper \n",
    "\n",
    "To fine tune contextual word embeddings we collect more (unlabeled) data from Reddit using the [Pushshift API](https://github.com/pushshift/api) through [PSAW](https://psaw.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3643f574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import time\n",
    "from psaw import PushshiftAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7cd3d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(action):\n",
    "    global time_passed\n",
    "    new_time = time.time()\n",
    "    print(action + f'took {(new_time - time_passed)} seconds')\n",
    "    time_passed = new_time\n",
    "    return\n",
    "\n",
    "\"\"\"\n",
    "Saves given post text to the specified file.\n",
    "\"\"\"\n",
    "def save_post(text, filename):\n",
    "    text = text.replace(\"\\n+\", \" \")\n",
    "    with open(filename, 'a', encoding=\"utf-8\") as f:\n",
    "        f.write(text + '\\n')\n",
    "        \n",
    "\"\"\"\n",
    "Saves the specified number of posts from the specified subreddit to the specified file. \n",
    "\"\"\"\n",
    "def scrape_subreddit(reddit, subname, filename, n_posts=10):\n",
    "    posts = api.search_submissions(subreddit=subname, mem_safe=True)\n",
    "    i = 0\n",
    "    for submission in posts: \n",
    "        try:\n",
    "            text = submission.selftext\n",
    "            if text and len(text) != 0 and '[removed]' not in text and '[deleted]' not in text:\n",
    "                save_post(text, filename)\n",
    "                i += 1\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        \n",
    "        if i >= n_posts:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6b5504",
   "metadata": {},
   "source": [
    "## Credentials\n",
    "Enter your own reddit app credentials instead of empty strings below. Follow [first steps guide by Reddit API](https://github.com/reddit-archive/reddit/wiki/OAuth2-Quick-Start-Example#first-steps) for more information about reddit app creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39ba1d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id='',\n",
    "                     client_secret='', \n",
    "                     password='',\n",
    "                     user_agent='', \n",
    "                     username='')\n",
    "api = PushshiftAPI(reddit)\n",
    "time_passed = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202351e5",
   "metadata": {},
   "source": [
    "## Scraping details\n",
    "Customize subreddits, scraped data destination and number of reddit posts below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c30fbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = [\n",
    "    'domesticviolence',\n",
    "    'survivorsofabuse', \n",
    "    'anxiety',\n",
    "    'stress',\n",
    "    'almosthomeless',\n",
    "    'assistance',\n",
    "    'food_pantry',\n",
    "    'homeless',\n",
    "    'ptsd',\n",
    "    'relationships'\n",
    "]\n",
    "FILE = 'scraped-data-all.txt' # scraped data destination\n",
    "N = 10000 # number of posts per subreddit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d10a0e",
   "metadata": {},
   "source": [
    "## Run scraping\n",
    "\n",
    "Depending on active PushShift shards the wanted number of posts may not always be attained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf9607af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\38599\\anaconda3\\lib\\site-packages\\psaw\\PushshiftAPI.py:252: UserWarning: Not all PushShift shards are active. Query results may be incomplete\n",
      "  warnings.warn(shards_down_message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "domesticviolence done took 261.1838638782501 seconds\n",
      "survivorsofabuse done took 86.72371578216553 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\38599\\anaconda3\\lib\\site-packages\\psaw\\PushshiftAPI.py:192: UserWarning: Got non 200 code 429\n",
      "  warnings.warn(\"Got non 200 code %s\" % response.status_code)\n",
      "C:\\Users\\38599\\anaconda3\\lib\\site-packages\\psaw\\PushshiftAPI.py:180: UserWarning: Unable to connect to pushshift.io. Retrying after backoff.\n",
      "  warnings.warn(\"Unable to connect to pushshift.io. Retrying after backoff.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anxiety done took 248.17567539215088 seconds\n",
      "stress done took 115.80576658248901 seconds\n",
      "almosthomeless done took 67.03820085525513 seconds\n",
      "assistance done took 790.1212203502655 seconds\n",
      "food_pantry done took 85.13964891433716 seconds\n",
      "homeless done took 323.1671495437622 seconds\n",
      "ptsd done took 316.2838931083679 seconds\n",
      "relationships done took 1053.020528793335 seconds\n"
     ]
    }
   ],
   "source": [
    "for sub in subreddits:\n",
    "    scrape_subreddit(reddit, sub, FILE, N)\n",
    "    log(f'{sub} done ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
